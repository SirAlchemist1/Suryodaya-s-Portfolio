<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Suryodaya Portfolio</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600&family=Playfair+Display:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/js/all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
    <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
</head>
<body>
    <div id="loading-screen">
        <div id="matrix-effect"></div>
        <div id="loading-text"></div>
    </div>

    <div id="particles-js"></div>

    <div id="scroll-progress"></div>

    <header>
        <nav class="navbar">
            <div class="logo">
                <div id="user-time"></div>
            </div>
            <ul class="nav-links">
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#awards">Awards</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
            <!-- Dark Mode Toggle Button -->
            <div class="theme-toggle" onclick="toggleTheme()">
                <svg class="theme-toggle-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
                <span class="theme-toggle-text">Dark</span>
            </div>
            <div class="burger">
                <div class="line1"></div>
                <div class="line2"></div>
                <div class="line3"></div>
            </div>
        </nav>
    </header>

    <main>
        <section id="home" class="reveal">
            <div id="home-particles"></div>
            <div class="container">
                <div class="home-content">
                    <div class="greeting animate-text">
                        <span class="greeting-text">Namaste! I'm</span>
                    </div>
                    <h1 class="name">
                        <span class="animate-text name-first">Suryodaya</span>
                        <span class="animate-text name-middle">Bikram</span>
                        <span class="animate-text name-last">Shahi</span>
                    </h1>
                    <div class="role-tags">
                        <span class="role-tag animate-text">Vision-Language Researcher</span>
                        <span class="role-tag animate-text">Wearable AI Engineer</span>
                        <span class="role-tag animate-text">Multimodal Learning</span>
                    </div>
                    <div class="typing-container">
                        <span class="typing-text" id="typing-text"></span>
                        <span class="typing-cursor">|</span>
                    </div>
                    <p class="motto animate-text">Sic Parvis Magna</p>
                    <div class="cta-buttons">
                        <a href="#about" class="btn primary-btn animate-btn">
                            <span class="btn-text">Explore My Work</span>
                            <i class="fas fa-arrow-right btn-icon"></i>
                        </a>
                        <a href="#contact" class="btn secondary-btn animate-btn">
                            <span class="btn-text">Get in Touch</span>
                            <i class="fas fa-envelope btn-icon"></i>
                        </a>
                        <a href="#" class="btn download-btn animate-btn" onclick="openResumeModal()">
                            <span class="btn-text">View Résumé</span>
                            <i class="fas fa-file-pdf btn-icon"></i>
                        </a>
                    </div>
                </div>
                <div class="social-icons">
                    <a href="https://github.com/SirAlchemist1" target="_blank" class="animate-icon social-link github-link" aria-label="GitHub">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-github"></i>
                            <span class="social-tooltip">GitHub</span>
                        </div>
                    </a>
                    <a href="https://www.linkedin.com/in/suryodaya-bikram-shahi-051a4b234?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B%2FV1W%2By3DTIC%2Bm8HbWyE%2FCQ%3D%3D" target="_blank" class="animate-icon social-link linkedin-link" aria-label="LinkedIn">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-linkedin"></i>
                            <span class="social-tooltip">LinkedIn</span>
                        </div>
                    </a>
                    <a href="https://x.com/SuryodayaShahi" target="_blank" class="animate-icon social-link twitter-link" aria-label="X (Twitter)">
                        <div class="social-icon-wrapper">
                            <svg width="1.3em" height="1.3em" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align:middle;"><path d="M299.5 0h220.6l180.6 273.2L885.2 0H1200L753.7 623.6 1200 1227H979.4L779.2 972.2 564.7 1227H250.2l464.2-627.2L0 0h299.5Zm77.2 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6ZM180.6 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6Z" fill="currentColor"/></svg>
                            <span class="social-tooltip">X (Twitter)</span>
                        </div>
                    </a>
                </div>
            </div>
        </section>

        <section id="about" class="reveal">
            <div class="container">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <div class="about-image">
                        <img src="images/profilepic-image.jpg" alt="Suryodaya Bikram Shahi">
                    </div>
                    <div class="about-text">
                        <p>
                          Namaste! I'm <strong>Suryodaya Bikram Shahi</strong> — born in Nepal, driven by the belief that AI should serve people first.  
                        </p>

                        <p>
                          At the <strong>University of Maryland</strong>, I'm finishing my M.S. in Data Science while working on <strong>VioPose</strong> (audiovisual 4D human pose for violin performance) with the Perception & Robotics Group. As a <strong>Perplexity.ai Campus Partner</strong>, I lead AI research initiatives and student engagement programs. This follows my summer at <strong>Harvard Ophthalmology AI & Robotics Lab</strong>, where I deployed VLMs on Meta Aria Gen-1 glasses and co-designed <strong>VISTA</strong> for assistive action evaluation.  
                        </p>

                        <p>
                          From <strong>real-time VLM pipelines for smart glasses</strong> to <strong>self-improving tiny language models</strong>, I work on building assistive AI at the edge. My research focuses on vision-language systems, on-device perception, and multimodal learning for accessibility. As a <strong>Perplexity.ai Campus Partner</strong>, I bridge cutting-edge AI research with practical student applications.  
                        </p>

                        <p>
                          My compass is simple: use technology to <em>expand access, dignity, and opportunity</em> — whether in a rural school, a hospital ward, or a research lab.  
                        </p>
                    </div>
                </div>
                <div class="about-cards">
                    <div class="flip-card" data-aos="fade-up">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-brain"></i>
                                <h3>Research Focus</h3>
                            </div>
                            <div class="flip-card-back">
                                <ul>
                                    <li>Vision–Language for Assistive Tech</li>
                                    <li>Wearable AI (Smart Glasses)</li>
                                    <li>Multimodal Perception</li>
                                    <li>Human Motion & Pose</li>
                                    <li>Low-latency On-device ML</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flip-card" data-aos="fade-up" data-aos-delay="100">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-microscope"></i>
                                <h3>Current Projects</h3>
                            </div>
                            <div class="flip-card-back">
                                <ul>
                                    <li><strong>VioPose</strong>: Audiovisual 4D human pose</li>
                                    <li><strong>VISTA</strong>: Action-grounded egocentric dataset</li>
                                    <li><strong>Aria-Qwen</strong>: Real-time VLM pipeline</li>
                                    <li><strong>Tiny-ACE</strong>: Self-improving tiny LMs</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flip-card" data-aos="fade-up" data-aos-delay="200">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-graduation-cap"></i>
                                <h3>Academic Journey</h3>
                            </div>
                            <div class="flip-card-back">
                                <h4>M.S. Data Science (UMD)</h4>
                                <p>Expected Dec 2025</p>
                                <ul>
                                    <li>Perception & Robotics Group</li>
                                    <li>Harvard Ophthalmology AI Lab</li>
                                    <li>Macquarie University Research</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="education" class="reveal">
            <div class="container">
                <h2>Education</h2>
                <div class="education-grid">
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/umd_logo.png" alt="University of Maryland Logo">
                                </div>
                                <h3>University of Maryland, College Park</h3>
                                <p>M.S. in Data Science</p>
                                <p>Expected Graduation: Dec 2025</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>M.S. Data Science Program</h4>
                                <h5>Key Achievements:</h5>
                                <ul>
                                    <li>Research focus on Computer Vision & AI</li>
                                    <li>Active in Perception & Robotics Group</li>
                                    <li>Collaborative research with Harvard</li>
                                </ul>
                                <h5>Relevant Coursework:</h5>
                                <ul>
                                    <li>Advanced Machine Learning</li>
                                    <li>Computer Vision & Image Processing</li>
                                    <li>Natural Language Processing (NLP)</li>
                                    <li>Big Data Systems & Analytics</li>
                                    <li>Statistical Methods & Probability</li>
                                    <li>Data Representation and Modeling</li>
                                    <li>Algorithms for Data Science</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/dtu_logo.png" alt="Delhi Technological University Logo">
                                </div>
                                <h3>Delhi Technological University</h3>
                                <p>B.Tech in Software Engineering</p>
                                <p>Graduated: May 2023</p>
                                <p>GPA: 8.22/10.0</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>B.Tech Software Engineering</h4>
                                <h5>Academic Excellence:</h5>
                                <ul>
                                    <li>GPA: 8.22/10.0 (Distinction)</li>
                                    <li>Strong foundation in CS fundamentals</li>
                                    <li>Project-based learning approach</li>
                                </ul>
                                <h5>Core Coursework:</h5>
                                <ul>
                                    <li>Software Engineering & Design</li>
                                    <li>Data Structures & Algorithms</li>
                                    <li>Computer Networks & Security</li>
                                    <li>Database Management Systems</li>
                                    <li>Web Development & Technologies</li>
                                    <li>Operating Systems & Architecture</li>
                                    <li>Object-Oriented Programming</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                    <!-- Harvard Continuing Education Card -->
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/Harvard DCE.png" alt="Harvard University Logo">
                                </div>
                                <h3>Harvard University</h3>
                                <p>Division of Continuing Education</p>
                                <p>Concurrent Enrollment: Summer Semester</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>Harvard Extension Studies</h4>
                                <h5>Advanced Coursework:</h5>
                                <ul>
                                    <li>DGMD S-14: Wearable Devices & Computer Vision</li>
                                    <li>Focus on AI & Human-Computer Interaction</li>
                                    <li>Research collaboration opportunities</li>
                                </ul>
                                <h5>Academic Details:</h5>
                                <ul>
                                    <li>Professor: Nabib Ahmed</li>
                                    <li>Graduate-level curriculum</li>
                                    <li>Concurrent with UMD studies</li>
                                    <li>Cross-institutional research</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="experience" class="reveal">
            <div class="container">
                <h2>Professional Experience</h2>
                <div class="timeline">
                    <!-- PRG/UMD Research Assistant -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/umd_logo.png" alt="UMD PRG" class="timeline-logo" />
                                <div>
                                    <h3>Research Assistant <span class="badge">Part-time</span></h3>
                                    <h4>
                                        <a href="https://robotics.umd.edu/facilities/perception-and-robotics-group" target="_blank">
                                            Perception & Robotics Group, University of Maryland
                                        </a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> College Park, MD</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Sept 2025 – Present</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Enhancing <strong>VioPose</strong> (audiovisual 4D pose) for violin performance; +12–15% pose fidelity.</li>
                                <li class="visible">Synced and preprocessed <strong>120+</strong> multimodal recordings (VioDat) for calibrated SMPL-X retargeting.</li>
                                <li class="visible">Vector clean-up for 3D finger joints → <strong>−30%</strong> drift; evaluating RoHM, FinePOSE, NLF on 3k+ frames.</li>
                                <li class="hidden">Prototyping an LLM-based auditory embedding to couple violin acoustics with fine-grained pose cues.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    <!-- Harvard Experience -->
                    <div class="timeline-item" data-aos="fade-left">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/Harvard AI.jpg" alt="Harvard Logo" class="timeline-logo" />
                                <div>
                                    <h3>AI Research Intern <span class="badge">Full Time</span></h3>
                                    <h4>
                                        <a href="https://wang.hms.harvard.edu/team/suryodaya-bikram-shahi/" target="_blank">Harvard Ophthalmology AI & Robotics Lab</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Boston, MA</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> May 2025 – Sept 2025</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Deployed VLMs (DeepSeek, Qwen-VL, LLaVA) on Meta Aria Gen-1; Qwen-VL best under motion blur (<strong>+23%</strong> grounding).</li>
                                <li class="visible">Co-designed <strong>VISTA</strong>: 1000+ real scenarios + <em>Action-Truth</em> labels for assistive action evaluation (targets: <em>npj Digital Medicine</em>, <em>The Lancet Digital Health</em>).</li>
                                <li class="hidden">Researching Vision-Language-Action (VLA) methods and techniques to improve the actions of a robotic surgical arm (Unitree) for medical applications.</li>
                                <li class="hidden">Implementing advanced deep learning models for real-time object detection and scene understanding in assistive technology contexts.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    
                    <!-- DTU Experience -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/dtu_logo.png" alt="DTU Logo" class="timeline-logo" />
                                <div>
                                    <h3>Undergraduate Research <span class="badge">Part-time</span></h3>
                                    <h4>
                                        <a href="https://www.dtu.ac.in/" target="_blank">Delhi Technological University</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> New Delhi, India</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Jan 2022 – Jun 2023</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Co-authored Springer ICICNIS chapter on <strong>vehicle trajectory prediction</strong> (CNN-LSTM; <strong>+14%</strong> accuracy).</li>
                                <li class="visible">Surveyed 43+ deep models; identified gaps and benchmark needs for multimodal spatiotemporal learning.</li>
                                <li class="hidden">Developed deep learning models for autonomous vehicle trajectory prediction using CNN-LSTM architectures.</li>
                                <li class="hidden">Conducted comprehensive literature review on deep learning methods for IoT and autonomous systems.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    
                    <!-- Macquarie University Experience -->
                    <div class="timeline-item" data-aos="fade-left">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/Macquarie.png" alt="Macquarie Logo" class="timeline-logo" />
                                <div>
                                    <h3>Research Intern <span class="badge">Remote</span></h3>
                                    <h4>
                                        <a href="https://www.mq.edu.au/" target="_blank">Macquarie University</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Sydney, Australia</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Jul 2023 – Oct 2023</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Processed & annotated <strong>5,000+</strong> Nepali memes (code-mix/switch; hate, humor, positivity).</li>
                                <li class="visible">Built BERT, VGG19, ResNet50; improved hateful-content detection by <strong>+17%</strong>.</li>
                                <li class="hidden">Optimized a large-scale multilingual meme dataset for sentiment analysis, reducing processing time by 60% and enhancing model efficiency.</li>
                                <li class="hidden">Integrated multimodal approaches (VGG16 + BERT), boosting hateful content detection accuracy by 40% compared to single-modality approaches.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    <!-- Nepal Electricity Authority Experience -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/NEA.png" alt="NEA Logo" class="timeline-logo" />
                                <div>
                                    <h3>IT Intern <span class="badge">On-site</span></h3>
                                    <h4>
                                        <a href="https://nea.org.np/" target="_blank">Nepal Electricity Authority (Government Entity)</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Kathmandu, Nepal</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> May 2022 – July 2022</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Centralized inventory operations with predictive analytics, optimizing accuracy and resource distribution across multiple power stations.</li>
                                <li class="visible">Created predictive model using classification algorithms like Random Forest and resource allocation on a Kaggle dataset for demand forecasting.</li>
                                <li class="visible">Developed data-driven solutions for optimizing electrical grid operations and improving energy distribution efficiency.</li>
                                <li class="hidden">Applied K-Means Clustering techniques to enhance inventory management, improving process efficiency by 50% and reducing operational costs.</li>
                                <li class="hidden">Implemented machine learning models for predictive maintenance of electrical infrastructure, reducing downtime and improving service reliability.</li>
                                <li class="hidden">Collaborated with engineering teams to integrate AI solutions into existing power grid management systems for real-time monitoring and control.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="projects" class="reveal">
            <div class="container">
                <h2>Current Projects</h2>
                <p class="projects-intro">Building the future of assistive AI through cutting-edge research in vision-language systems, wearable technology, and edge computing. Each project represents a step toward more accessible and intelligent human-computer interaction.</p>
                <div class="project-grid">
                    <!-- VioPose Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-image-container">
                            <img src="images/VioPose.png" alt="VioPose: Violin Performance 4D Pose Estimation" class="project-main-image">
                        </div>
                        <div class="project-header">
                            <h4 class="project-title">VioPose: Violin Performance 4D Pose Estimation</h4>
                            <p class="project-subtitle">Hierarchical Audiovisual Inference for Precise Motion Analysis</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">University of Maryland</span>
                            <span class="lab">Perception & Robotics Group (Prof. Cornelia Fermüller)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Multimodal Learning</span>
                            <span class="tag">4D Pose Estimation</span>
                            <span class="tag">Audiovisual Fusion</span>
                            <span class="tag">Human Motion</span>
                            <span class="tag">Computer Vision</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>Largest calibrated violin-playing dataset</strong> with video, audio, and 3D motion capture. 
                            Hierarchical multimodal network estimating <strong>fine-grained motions</strong> (vibrato ≈ 10mm) and 
                            <strong>large motions</strong> (bowing) simultaneously, outperforming SoTA visual-only methods.
                        </div>
                        
                        <p class="project-description">
                            Novel multimodal network that hierarchically estimates 4D human pose (3D pose over time) for violin performance 
                            by leveraging the direct causal relationship between music and human motions. Addresses challenges of occlusions, 
                            partial views, and fast subtle movements (e.g., vibrato) that visual-only methods fail to capture. High-level 
                            features cascade to low-level features with Bayesian updates, producing accurate pose sequences for precise 
                            motion analysis. Published at WACV 2025.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>Outperforms state-of-the-art visual-only pose estimation methods</li>
                                    <li>Successfully captures fine-grained vibrato motion (≈10mm perturbation) and large bowing motions</li>
                                    <li>Collected largest and most diverse calibrated violin-playing dataset (12 performers, 4 camera views)</li>
                                    <li>Hierarchical architecture with single-modality encoders, hierarchy module, and mixing module</li>
                                    <li>Code and dataset released for research community benefit</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://sj-yoo.info/viopose/" class="btn-link" target="_blank">
                                <i class="fas fa-globe"></i> Project Page
                            </a>
                            <a href="https://github.com/SirAlchemist1" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> Code
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-file-alt"></i> Paper (WACV 2025)
                            </a>
                        </div>
                    </div>
                    
                    <!-- Aria Glasses Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">Aria Glasses on Qwen</h4>
                            <p class="project-subtitle">Real-Time Vision-Language Pipeline for Meta Aria Gen-1</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">Harvard Medical School</span>
                            <span class="lab">AI & Robotics Lab (Dr. Mengyu Wang)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Wearable AI</span>
                            <span class="tag">Vision-Language Models</span>
                            <span class="tag">On-Device Inference</span>
                            <span class="tag">Assistive Technology</span>
                            <span class="tag">Real-Time Processing</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>35% faster inference</strong> compared to baseline implementations, 
                            <strong>60% fewer manual inputs</strong> through intelligent context-aware responses. 
                            Optimized for low-latency on-device inference on Meta Aria Gen-1 smart glasses.
                        </div>
                        
                        <p class="project-description">
                            Real-time Vision-Language Model (VLM) pipeline for Meta Aria Gen-1 smart glasses integrating Qwen-VL and BLIP 
                            for scene understanding and assistive AI applications. Designed for assistive technology use cases requiring 
                            real-time visual understanding and natural language interaction. Optimized for low-latency on-device inference, 
                            enabling seamless user experience without cloud dependency.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>35% faster processing compared to baseline VLM implementations</li>
                                    <li>60% reduction in manual user inputs through context-aware responses</li>
                                    <li>Optimized for on-device inference without cloud dependency</li>
                                    <li>Successfully deployed on Meta Aria Gen-1 hardware</li>
                                    <li>Open-source implementation available on GitHub</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/SirAlchemist1/Aria-glasses-on-Qwen" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </div>
                    </div>
                    
                    <!-- Tiny-ACE Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">Tiny-ACE: Self-Improving Tiny LMs</h4>
                            <p class="project-subtitle">Domain-Specific Benchmarking with Agentic Context Engineering</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">Research Project</span>
                            <span class="lab">Edge AI & Language Models</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Small Language Models</span>
                            <span class="tag">Agentic Context Engineering</span>
                            <span class="tag">Edge Computing</span>
                            <span class="tag">Domain Adaptation</span>
                            <span class="tag">Self-Learning</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>No fine-tuning required</strong>—adapt frozen SLMs using ACE-style playbook memory. 
                            Benchmarked on <strong>TAT-QA (finance)</strong> and <strong>MedQA (medical)</strong> tasks. 
                            Optimized for Mac M-series, consumer GPUs, and compute-constrained environments.
                        </div>
                        
                        <p class="project-description">
                            Domain-specific benchmarking framework for Small Language Models (SLMs) on edge devices using 
                            <strong>Agentic Context Engineering (ACE)</strong> instead of traditional fine-tuning. Evaluates 
                            sub-3B models (Phi-2, TinyLlama) on specialized tasks using self-improving playbook memory (1–4k tokens). 
                            Implements SEAL/SPICE-inspired algorithms for automatic prompt evolution, enabling on-device continual 
                            learning without parameter updates.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>No fine-tuning—adapt frozen SLMs via dynamic playbook memory</li>
                                    <li>Benchmarked on domain-specific tasks (TAT-QA finance, MedQA medical)</li>
                                    <li>Self-improving playbook memory (1–4k tokens) for on-device learning</li>
                                    <li>SEAL/SPICE-inspired algorithms for automatic prompt evolution</li>
                                    <li>Optimized for Mac M-series and consumer GPUs</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/SirAlchemist1/edge-slm-ace" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                            <a href="https://arxiv.org/abs/2510.04618" class="btn-link" target="_blank">
                                <i class="fas fa-file-alt"></i> ArXiv
                            </a>
                        </div>
                    </div>
                    
                    <!-- HC-TAP Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">HC-TAP: Healthcare Text Analytics</h4>
                            <p class="project-subtitle">Serverless NLP Pipeline for Clinical Notes Entity Extraction</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">AWS Serverless Architecture</span>
                            <span class="lab">Healthcare Analytics Platform</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">NLP</span>
                            <span class="tag">AWS Serverless</span>
                            <span class="tag">Healthcare Analytics</span>
                            <span class="tag">Entity Extraction</span>
                            <span class="tag">Streamlit Dashboard</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>−40% analyst time</strong> via automated entity extraction and searchable dashboards. 
                            Extracts medical entities (Problems/Diagnoses, Medications, Tests) from free-text clinical notes 
                            using AWS Comprehend Medical with Streamlit analytics dashboard.
                        </div>
                        
                        <p class="project-description">
                            Serverless NLP pipeline extracting medical entities from free-text clinical notes using AWS Comprehend Medical. 
                            Architecture: S3 → Lambda → Comprehend Medical → JSONL storage → Athena DDL for SQL queries. Built Streamlit 
                            analytics dashboard with KPIs (latency p50/p95, error rates), top-10 entity rankings, and CSV exports. 
                            Makes clinical notes queryable and dashboard-ready without manual chart skimming.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>40% reduction in analyst time via automated entity extraction</li>
                                    <li>Streamlit dashboard with KPIs, top entities, and CSV exports</li>
                                    <li>Serverless architecture: Lambda, S3, Comprehend Medical, Athena</li>
                                    <li>Extracts Problems/Diagnoses, Medications, and Tests entities</li>
                                    <li>Athena DDL prepared for SQL queries over JSONL data lake</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/withsivram/hc-tap" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </div>
                    </div>
                    
                    <!-- VISTA Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-image-container">
                            <img src="images/VISTA.png" alt="VISTA: Assistive Vision Dataset" class="project-main-image">
                        </div>
                        <div class="project-header">
                            <h4 class="project-title">VISTA: Assistive Vision Dataset</h4>
                            <p class="project-subtitle">Multimodal Egocentric Dataset for Low-Vision AI</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">Harvard Medical School</span>
                            <span class="lab">AI & Robotics Lab (Dr. Mengyu Wang)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Assistive AI</span>
                            <span class="tag">Dataset Engineering</span>
                            <span class="tag">Wearable Sensing</span>
                            <span class="tag">Privacy-by-Design</span>
                            <span class="tag">Human-Centered AI</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>1,000+ egocentric recordings</strong> from Meta Project Aria glasses, 
                            <strong>9 task categories</strong> (navigation, hazard detection, text understanding), 
                            <strong>6 multimodal streams</strong> with &lt;1ms synchronization.
                        </div>
                        
                        <p class="project-description">
                            Led data collection and engineering design of the first multimodal dataset specifically designed 
                            for assistive AI benchmarking with visually impaired users. Coordinated egocentric multimodal 
                            capture (RGB video, IMU, spatial audio, SLAM, eye-tracking), built preprocessing pipeline with 
                            privacy-by-design (automated face/license plate blurring), and developed chunked loading architecture 
                            for efficient handling of 4GB+ .vrs files.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>Sub-second on-device inference latency (NVIDIA Jetson)</li>
                                    <li>Harvard IRB compliance with ethical safeguards throughout workflow</li>
                                    <li>Adaptive preprocessing (gamma correction, exposure balancing) for lighting variability</li>
                                    <li>Custom Label Studio interfaces for action-level ground truth annotation</li>
                                    <li>Planned open-source release on HuggingFace for community benchmarking</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/Harvard-AI-and-Robotics-Lab" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> Lab GitHub
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-file"></i> Technical Report
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-database"></i> Dataset (HuggingFace)
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="skills" class="reveal">
            <div id="skills-particles"></div>
            <div class="container">
                <h2>My Skills</h2>
                <div class="skills-filter-tabs">
                    <button class="filter-tab active" data-category="all">All</button>
                    <button class="filter-tab" data-category="programming">Programming</button>
                    <button class="filter-tab" data-category="advanced-ml">ML/AI</button>
                    <button class="filter-tab" data-category="data-tools">Data Tools</button>
                    <button class="filter-tab" data-category="robotics-vision">Robotics</button>
                    <button class="filter-tab" data-category="research-methods">Research</button>
                    <button class="filter-tab" data-category="academic-writing">Academic</button>
                </div>
                <div class="skills-grid">
                    <div class="skill-hex" data-category="programming">
                        <div class="hexagon">
                            <i class="fas fa-code"></i>
                        </div>
                        <h3>Programming Languages</h3>
                    </div>
                    <div class="skill-hex" data-category="datascience">
                        <div class="hexagon">
                            <i class="fas fa-brain"></i>
                        </div>
                        <h3>Data Science & ML</h3>
                    </div>
                    <div class="skill-hex" data-category="tools">
                        <div class="hexagon">
                            <i class="fas fa-tools"></i>
                        </div>
                        <h3>Tools & Technologies</h3>
                    </div>
                    <div class="skill-hex" data-category="software">
                        <div class="hexagon">
                            <i class="fas fa-laptop-code"></i>
                        </div>
                        <h3>Software Development</h3>
                    </div>
                    <div class="skill-hex" data-category="research">
                        <div class="hexagon">
                            <i class="fas fa-search"></i>
                        </div>
                        <h3>Research & Analysis</h3>
                    </div>
                    <div class="skill-hex" data-category="collaboration">
                        <div class="hexagon">
                            <i class="fas fa-users"></i>
                        </div>
                        <h3>Collaboration & Leadership</h3>
                    </div>
                    <div class="skill-hex" data-category="advanced-ml">
                        <div class="hexagon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <h3>Advanced AI/ML & Robotics</h3>
                    </div>
                    <div class="skill-hex" data-category="data-tools">
                        <div class="hexagon">
                            <i class="fas fa-database"></i>
                        </div>
                        <h3>Data & Databases</h3>
                    </div>
                    <div class="skill-hex" data-category="research-methods">
                        <div class="hexagon">
                            <i class="fas fa-microscope"></i>
                        </div>
                        <h3>Research Methodologies</h3>
                    </div>
                    <div class="skill-hex" data-category="academic-writing">
                        <div class="hexagon">
                            <i class="fas fa-pen-fancy"></i>
                        </div>
                        <h3>Academic Writing</h3>
                    </div>
                </div>
            </div>
            <div id="skill-details" class="skill-details">
                <h3 id="skill-category-title"></h3>
                <ul id="skill-list"></ul>
                <button id="close-skill-details">&times;</button>
            </div>
        </section>

        <section id="awards" class="reveal">
            <div class="container">
                <h2>Awards & Recognition</h2>
                <div class="awards-carousel">
                    <div class="award-badge featured" data-aos="fade-up">
                        <div class="badge-icon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <div class="badge-content">
                            <h3>Perplexity.ai Campus Partner</h3>
                            <p>University of Maryland, College Park</p>
                            <span class="badge-detail">Leading AI Research & Student Engagement</span>
                            <a href="https://www.perplexity.ai/studentonboarding" target="_blank" class="badge-link">View Program</a>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="100">
                        <div class="badge-icon">
                            <i class="fas fa-user-check"></i>
                        </div>
                        <div class="badge-content">
                            <h3>ACM TheWebConf 2025 Reviewer</h3>
                            <p>Workshop MM4SG</p>
                            <span class="badge-detail">2 Papers Reviewed</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="200">
                        <div class="badge-icon">
                            <i class="fas fa-graduation-cap"></i>
                        </div>
                        <div class="badge-content">
                            <h3>ICCR Scholar</h3>
                            <p>2019–2023</p>
                            <span class="badge-detail">Full Scholarship (DTU)</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="300">
                        <div class="badge-icon">
                            <i class="fas fa-rocket"></i>
                        </div>
                        <div class="badge-content">
                            <h3>CTO - Kaushala</h3>
                            <p>EdTech Startup</p>
                            <span class="badge-detail">6 School Partnerships</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="400">
                        <div class="badge-icon">
                            <i class="fas fa-medal"></i>
                        </div>
                        <div class="badge-content">
                            <h3>Gold Medalist</h3>
                            <p>TechFest IIT Bombay</p>
                            <span class="badge-detail">Science Olympiad 2018</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="publications" class="reveal">
            <div class="container">
                <h2>Research Publications & Presentations</h2>
                <p class="publications-intro">Advancing the frontiers of AI through peer-reviewed research in computer vision, multimodal learning, and assistive technologies. My work contributes to both academic knowledge and practical applications for accessibility and human-computer interaction.</p>
                <div class="research-highlights">
                    <div class="highlight-card">
                        <i class="fas fa-chart-line"></i>
                        <span>+14% accuracy improvement</span>
                        </div>
                    <div class="highlight-card">
                        <i class="fas fa-users"></i>
                        <span>3+ research collaborations</span>
                    </div>
                    <div class="highlight-card">
                        <i class="fas fa-graduation-cap"></i>
                        <span>2 papers in preparation</span>
                        </div>
                    </div>
                    
                <div class="publications-grid">
                    <div class="publication-category">
                        <h3><i class="fas fa-file-alt"></i> Published Papers</h3>
                        <div class="publication-list">
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>Deep Learning Methods for Vehicle Trajectory Prediction</h4>
                                    <span class="publication-status">Published</span>
                                </div>
                                <p class="publication-authors">Suryodaya Basak, Shuvam Shiwakoti, Priya Singh</p>
                                <p class="publication-venue">Springer ICICNIS 2023 · DOI: 10.1007/978-981-99-6586-1_37</p>
                                <div class="publication-links">
                                    <a href="https://link.springer.com/chapter/10.1007/978-981-99-6586-1_37" target="_blank" class="paper-link">Springer Chapter</a>
                                </div>
                                <div class="publication-abstract">
                                    <p>Co-authored paper on deep learning and IoT integration for autonomous vehicles. CNN-LSTM approach achieved +14% accuracy in trajectory prediction.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VTP.png" alt="Deep Learning Methods for Vehicle Trajectory Prediction" style="width: 100%; height: 200px; object-fit: cover;">
                                    <div class="preview-overlay">
                                        <a href="https://link.springer.com/chapter/10.1007/978-981-99-6586-1_37" target="_blank" class="preview-link">
                                            <i class="fas fa-external-link-alt"></i>
                                            View Paper
                                        </a>
                                    </div>
                                    <p class="preview-caption">Deep Learning Methods for Vehicle Trajectory Prediction - Springer ICICNIS 2023</p>
                                </div>
                        </div>
                        </div>
                    </div>
                    
                    <div class="publication-category">
                        <h3><i class="fas fa-microphone"></i> In Preparation</h3>
                        <div class="publication-list">
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>VioPose++: Audiovisual Human–Instrument Motion</h4>
                                    <span class="publication-status">In Prep</span>
                                </div>
                                <p class="publication-authors">With SeongJong Yoo, Snehesh Shrestha (PI: Prof. R. Fermüller)</p>
                                <p class="publication-venue">Target: CVPR 2026</p>
                                <div class="publication-abstract">
                                    <p>Enhancing VioPose (audiovisual 4D pose) for violin performance; <strong>+12–15% pose fidelity</strong> with calibrated SMPL-X retargeting.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VioPose.png" alt="VioPose Research Visualization" style="width: 100%; height: 200px; object-fit: cover;">
                                    <div class="preview-overlay">
                                        <a href="#" class="preview-link" onclick="showComingSoon()">
                                            <i class="fas fa-file-alt"></i>
                                            Coming Soon
                                        </a>
                                    </div>
                                    <p class="preview-caption">VioPose: Audiovisual 4D Human Pose for Violin Performance</p>
                                </div>
                            </div>
                    
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>VISTA: Action-Grounded Egocentric Dataset for Assistive AI</h4>
                                    <span class="publication-status">In Prep</span>
                                </div>
                                <p class="publication-authors">With Josh Yeh, Mengyu Wang (MEEI/Harvard)</p>
                                <p class="publication-venue">Targets: <em>npj Digital Medicine</em> or <em>The Lancet Digital Health</em></p>
                                <div class="publication-abstract">
                                    <p><strong>1000+ real scenarios</strong> + <em>Action-Truth</em> labels for assistive action evaluation using Meta Aria Gen-1 glasses.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VISTA.png" alt="VISTA Dataset Visualization" style="width: 100%; height: 200px; object-fit: cover;">
                                    <div class="preview-overlay">
                                        <a href="#" class="preview-link" onclick="showComingSoon()">
                                            <i class="fas fa-file-alt"></i>
                                            Coming Soon
                                        </a>
                                    </div>
                                    <p class="preview-caption">VISTA: Action-Grounded Egocentric Dataset for Assistive AI</p>
                                </div>
                            </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="contact" class="reveal">
            <div class="container">
                <h2 class="section-title">Let's Connect</h2>
                <p class="contact-intro">Interested in collaborating on AI research, discussing assistive technologies, or exploring opportunities in vision-language systems? I'd love to hear from you.</p>
                <div class="contact-wrapper">
                    <div class="contact-info">
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fas fa-envelope"></i>
                            </div>
                            <div class="contact-details">
                                <h3>Email</h3>
                                <p><a href="mailto:sshahi20@umd.edu">sshahi20@umd.edu</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fas fa-phone"></i>
                            </div>
                            <div class="contact-details">
                                <h3>Phone</h3>
                                <p><a href="tel:+16674457800">(667) 445-7800</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fab fa-linkedin"></i>
                            </div>
                            <div class="contact-details">
                                <h3>LinkedIn</h3>
                                <p><a href="https://www.linkedin.com/in/suryodaya-bikram-shahi" target="_blank">Suryodaya Bikram Shahi</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fab fa-github"></i>
                            </div>
                            <div class="contact-details">
                                <h3>GitHub</h3>
                                <p><a href="https://github.com/SirAlchemist1" target="_blank">SirAlchemist1</a></p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="footer-content">
            <div class="footer-contact-card">
                <div class="footer-avatar">
                    <img src="images/profilepic-image.jpg" alt="Suryodaya Bikram Shahi">
                </div>
                <div class="footer-info">
                    <h3>Suryodaya Bikram Shahi</h3>
                    <p class="footer-tagline">Let's build something impactful together.</p>
                    <div class="footer-contact-details">
                        <div class="contact-item">
                            <i class="fas fa-envelope"></i>
                            <a href="mailto:sshahi20@umd.edu">sshahi20@umd.edu</a>
                        </div>
                        <div class="contact-item">
                            <i class="fas fa-map-marker-alt"></i>
                            <span>College Park, MD</span>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2024 Suryodaya Bikram Shahi. All rights reserved.</p>
                <div class="social-icons">
                    <a href="https://github.com/SirAlchemist1" target="_blank" class="social-link github-link" aria-label="GitHub">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-github"></i>
                            <span class="social-tooltip">GitHub</span>
                        </div>
                    </a>
                    <a href="https://www.linkedin.com/in/suryodaya-bikram-shahi-051a4b234?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B%2FV1W%2By3DTIC%2Bm8HbWyE%2FCQ%3D%3D" target="_blank" class="social-link linkedin-link" aria-label="LinkedIn">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-linkedin"></i>
                            <span class="social-tooltip">LinkedIn</span>
                        </div>
                    </a>
                    <a href="https://x.com/SuryodayaShahi" target="_blank" class="social-link twitter-link" aria-label="X (Twitter)">
                        <div class="social-icon-wrapper">
                            <svg width="1.1em" height="1.1em" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align:middle;"><path d="M299.5 0h220.6l180.6 273.2L885.2 0H1200L753.7 623.6 1200 1227H979.4L779.2 972.2 564.7 1227H250.2l464.2-627.2L0 0h299.5Zm77.2 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6ZM180.6 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6Z" fill="currentColor"/></svg>
                            <span class="social-tooltip">X (Twitter)</span>
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <div id="floating-action-button">
        <i class="fas fa-arrow-up"></i>
    </div>

    <div class="modal" id="project-modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2 id="modal-title"></h2>
            <p id="modal-description"></p>
            <a id="modal-link" href="#" target="_blank" class="btn primary-btn">View on GitHub</a>
        </div>
    </div>

    <div class="modal" id="resume-modal">
        <div class="modal-content resume-modal-content">
            <span class="close" onclick="closeResumeModal()">&times;</span>
            <h2>My Résumé</h2>
            <div class="resume-selector">
                <div class="resume-tabs">
                    <button class="resume-tab active" onclick="switchResume('academic')" id="academic-tab">
                        <i class="fas fa-graduation-cap"></i> Academic CV
                    </button>
                    <button class="resume-tab" onclick="switchResume('industry')" id="industry-tab">
                        <i class="fas fa-briefcase"></i> Industry Resume
                    </button>
                </div>
            </div>
            <div class="resume-options">
                <div class="resume-preview">
                    <embed id="resume-embed" src="PhD_application_CV_Template.pdf" type="application/pdf" width="100%" height="500px">
                </div>
                <div class="resume-actions">
                    <a id="download-link" href="PhD_application_CV_Template.pdf" class="btn primary-btn" download>
                        <i class="fas fa-download"></i> Download PDF
                    </a>
                    <a id="open-link" href="PhD_application_CV_Template.pdf" class="btn secondary-btn" target="_blank">
                        <i class="fas fa-external-link-alt"></i> Open in New Tab
                    </a>
                </div>
            </div>
        </div>
    </div>

    <script src="script.js"></script>
    
    <script>
    // Dark mode functionality
    function toggleTheme() {
        const body = document.body;
        const themeToggleText = document.querySelector('.theme-toggle-text');
        const themeToggleIcon = document.querySelector('.theme-toggle-icon');
        
        if (body.getAttribute('data-theme') === 'dark') {
            body.removeAttribute('data-theme');
            themeToggleText.textContent = 'Dark';
            themeToggleIcon.innerHTML = '<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>';
            localStorage.setItem('theme', 'light');
        } else {
            body.setAttribute('data-theme', 'dark');
            themeToggleText.textContent = 'Light';
            themeToggleIcon.innerHTML = '<circle cx="12" cy="12" r="5"></circle><path d="m12 1 0 6m0 6 0 6M4.22 4.22l1.42 1.42m11.72 11.72 1.42 1.42M1 12h6m6 0h6M4.22 19.78l1.42-1.42m11.72-11.72 1.42-1.42"></path>';
            localStorage.setItem('theme', 'dark');
        }
    }

    // Initialize theme - default to dark mode
    function initializeTheme() {
        const savedTheme = localStorage.getItem('theme');
        
        // Default to dark mode unless user has explicitly chosen light mode
        if (savedTheme !== 'light') {
            document.body.setAttribute('data-theme', 'dark');
            document.querySelector('.theme-toggle-text').textContent = 'Light';
            document.querySelector('.theme-toggle-icon').innerHTML = '<circle cx="12" cy="12" r="5"></circle><path d="m12 1 0 6m0 6 0 6M4.22 4.22l1.42 1.42m11.72 11.72 1.42 1.42M1 12h6m6 0h6M4.22 19.78l1.42-1.42m11.72-11.72 1.42-1.42"></path>';
            localStorage.setItem('theme', 'dark');
        }
    }

    // Initialize theme on page load
    document.addEventListener('DOMContentLoaded', initializeTheme);

    // Listen for system theme changes
    window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
        if (!localStorage.getItem('theme')) {
            if (e.matches) {
                document.body.setAttribute('data-theme', 'dark');
            } else {
                document.body.removeAttribute('data-theme');
            }
        }
    });

    // Resume switching functionality
    function switchResume(type) {
        const academicTab = document.getElementById('academic-tab');
        const industryTab = document.getElementById('industry-tab');
        const resumeEmbed = document.getElementById('resume-embed');
        const downloadLink = document.getElementById('download-link');
        const openLink = document.getElementById('open-link');
        
        // Update tab states
        if (type === 'academic') {
            academicTab.classList.add('active');
            industryTab.classList.remove('active');
            resumeEmbed.src = 'PhD_application_CV_Template.pdf';
            downloadLink.href = 'PhD_application_CV_Template.pdf';
            openLink.href = 'PhD_application_CV_Template.pdf';
        } else {
            industryTab.classList.add('active');
            academicTab.classList.remove('active');
            resumeEmbed.src = 'Industry resume .pdf';
            downloadLink.href = 'Industry resume .pdf';
            openLink.href = 'Industry resume .pdf';
        }
    }
    </script>
</body>
</html>