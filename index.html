<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Suryodaya Portfolio</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600&family=Playfair+Display:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/js/all.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
    <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
</head>
<body>
    <div id="loading-screen">
        <div id="matrix-effect"></div>
        <div id="loading-text"></div>
    </div>

    <div id="particles-js"></div>

    <div id="scroll-progress"></div>

    <header>
        <nav class="navbar">
            <div class="logo">
                <div id="user-time"></div>
            </div>
            <ul class="nav-links">
                <li><a href="#home">Home</a></li>
                <li><a href="#about">About</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#projects">Projects</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#awards">Awards</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
            <!-- Dark Mode Toggle Button -->
            <div class="theme-toggle" onclick="toggleTheme()">
                <svg class="theme-toggle-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
                <span class="theme-toggle-text">Dark</span>
            </div>
            <div class="burger">
                <div class="line1"></div>
                <div class="line2"></div>
                <div class="line3"></div>
            </div>
        </nav>
    </header>

    <main>
        <section id="home" class="reveal">
            <div id="home-particles"></div>
            <div class="container">
                <div class="home-content">
                    <div class="greeting animate-text">
                        <span class="greeting-text">Namaste! I'm</span>
                    </div>
                    <h1 class="name">
                        <span class="animate-text name-first">Suryodaya</span>
                        <span class="animate-text name-middle">Bikram</span>
                        <span class="animate-text name-last">Shahi</span>
                    </h1>
                    <div class="role-tags">
                        <span class="role-tag animate-text">Vision-Language Researcher</span>
                        <span class="role-tag animate-text">Wearable AI Engineer</span>
                        <span class="role-tag animate-text">Multimodal Learning</span>
                    </div>
                    <div class="typing-container">
                        <span class="typing-text" id="typing-text"></span>
                        <span class="typing-cursor">|</span>
                    </div>
                    <p class="motto animate-text">Sic Parvis Magna</p>
                    <div class="cta-buttons">
                        <a href="#about" class="btn primary-btn animate-btn">
                            <span class="btn-text">Explore My Work</span>
                            <i class="fas fa-arrow-right btn-icon"></i>
                        </a>
                        <a href="#contact" class="btn secondary-btn animate-btn">
                            <span class="btn-text">Get in Touch</span>
                            <i class="fas fa-envelope btn-icon"></i>
                        </a>
                        <a href="#" class="btn download-btn animate-btn" onclick="openResumeModal()">
                            <span class="btn-text">View Résumé</span>
                            <i class="fas fa-file-pdf btn-icon"></i>
                        </a>
                    </div>
                </div>
                <div class="social-icons">
                    <a href="https://github.com/SirAlchemist1" target="_blank" class="animate-icon social-link github-link" aria-label="GitHub">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-github"></i>
                            <span class="social-tooltip">GitHub</span>
                        </div>
                    </a>
                    <a href="https://www.linkedin.com/in/suryodaya-bikram-shahi-051a4b234?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B%2FV1W%2By3DTIC%2Bm8HbWyE%2FCQ%3D%3D" target="_blank" class="animate-icon social-link linkedin-link" aria-label="LinkedIn">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-linkedin"></i>
                            <span class="social-tooltip">LinkedIn</span>
                        </div>
                    </a>
                    <a href="https://x.com/SuryodayaShahi" target="_blank" class="animate-icon social-link twitter-link" aria-label="X (Twitter)">
                        <div class="social-icon-wrapper">
                            <svg width="1.3em" height="1.3em" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align:middle;"><path d="M299.5 0h220.6l180.6 273.2L885.2 0H1200L753.7 623.6 1200 1227H979.4L779.2 972.2 564.7 1227H250.2l464.2-627.2L0 0h299.5Zm77.2 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6ZM180.6 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6Z" fill="currentColor"/></svg>
                            <span class="social-tooltip">X (Twitter)</span>
                        </div>
                    </a>
                </div>
            </div>
        </section>

        <section id="about" class="reveal">
            <div class="container">
                <h2 class="section-title">About Me</h2>
                <div class="about-content">
                    <div class="about-image">
                        <img src="images/profilepic-image.jpg" alt="Suryodaya Bikram Shahi">
                    </div>
                    <div class="about-text">
                        <p>
                          Namaste! I'm <strong>Suryodaya Bikram Shahi</strong>. I am a researcher and engineer driven by a single belief: AI should serve people first.
                        </p>

                        <p>
                          Currently, I am finishing my M.S. in Data Science at the <strong>University of Maryland</strong>, where I work with the Perception & Robotics Group on <strong>VioPose</strong> (audiovisual 4D human pose estimation). My research focuses on embodied AI at the edge building systems that can see, hear, and assist in real-time without relying on massive compute.
                        </p>

                        <p>
                          Previously, I was at the <strong>Harvard Ophthalmology AI & Robotics Lab</strong>, where I co-designed <strong>VISTA</strong>, a dataset for assistive action evaluation, and deployed Vision-Language Models on Meta Aria glasses. Whether I'm optimizing tiny language models or leading outreach as a <strong>Perplexity.ai Campus Partner</strong>, my compass is simple: using technology to expand access, dignity, and opportunity from a rural school to a high-tech lab.
                        </p>
                    </div>
                </div>
                <div class="about-cards">
                    <div class="flip-card" data-aos="fade-up">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-brain"></i>
                                <h3>Research Focus</h3>
                            </div>
                            <div class="flip-card-back">
                                <ul>
                                    <li>Vision–Language for Assistive Tech</li>
                                    <li>Wearable AI (Smart Glasses)</li>
                                    <li>Multimodal Perception</li>
                                    <li>Human Motion & Pose</li>
                                    <li>Low-latency On-device ML</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flip-card" data-aos="fade-up" data-aos-delay="100">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-microscope"></i>
                                <h3>Current Projects</h3>
                            </div>
                            <div class="flip-card-back">
                                <ul>
                                    <li><strong>VioPose</strong>: Audiovisual 4D human pose</li>
                                    <li><strong>VISTA</strong>: Action-grounded egocentric dataset</li>
                                    <li><strong>Aria-Qwen</strong>: Real-time VLM pipeline</li>
                                    <li><strong>TINYACE</strong>: Bounded working-memory context engineering</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                    <div class="flip-card" data-aos="fade-up" data-aos-delay="200">
                        <div class="flip-card-inner">
                            <div class="flip-card-front">
                                <i class="fas fa-graduation-cap"></i>
                                <h3>Academic Journey</h3>
                            </div>
                            <div class="flip-card-back">
                                <h4>M.S. Data Science (UMD)</h4>
                                <p>Expected Dec 2025</p>
                                <ul>
                                    <li>Perception & Robotics Group</li>
                                    <li>Harvard Ophthalmology AI Lab</li>
                                    <li>Macquarie University Research</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="education" class="reveal">
            <div class="container">
                <h2>Education</h2>
                <div class="education-grid">
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/umd_logo.png" alt="University of Maryland Logo">
                                </div>
                                <h3>University of Maryland, College Park</h3>
                                <p>M.S. in Data Science (GPA: 3.83/4.0)</p>
                                <p>Expected Graduation: Dec 2025</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>M.S. Data Science Program</h4>
                                <h5>Key Achievements:</h5>
                                <ul>
                                    <li>Research focus on Computer Vision & AI</li>
                                    <li>Active in Perception & Robotics Group</li>
                                    <li>Collaborative research with Harvard</li>
                                </ul>
                                <h5>Relevant Coursework:</h5>
                                <ul>
                                    <li>Advanced Machine Learning</li>
                                    <li>Computer Vision & Image Processing</li>
                                    <li>Natural Language Processing (NLP)</li>
                                    <li>Big Data Systems & Analytics</li>
                                    <li>Statistical Methods & Probability</li>
                                    <li>Data Representation and Modeling</li>
                                    <li>Algorithms for Data Science</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/dtu_logo.png" alt="Delhi Technological University Logo">
                                </div>
                                <h3>Delhi Technological University</h3>
                                <p>B.Tech in Software Engineering</p>
                                <p>Graduated: May 2023</p>
                                <p>GPA: 8.22/10.0</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>B.Tech Software Engineering</h4>
                                <h5>Academic Excellence:</h5>
                                <ul>
                                    <li>GPA: 8.22/10.0 (Distinction)</li>
                                    <li>Strong foundation in CS fundamentals</li>
                                    <li>Project-based learning approach</li>
                                </ul>
                                <h5>Core Coursework:</h5>
                                <ul>
                                    <li>Software Engineering & Design</li>
                                    <li>Data Structures & Algorithms</li>
                                    <li>Computer Networks & Security</li>
                                    <li>Database Management Systems</li>
                                    <li>Web Development & Technologies</li>
                                    <li>Operating Systems & Architecture</li>
                                    <li>Object-Oriented Programming</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                    <!-- Harvard Continuing Education Card -->
                    <div class="education-card">
                        <div class="card-inner">
                            <div class="card-front">
                                <div class="university-logo">
                                    <img src="images/Harvard%20DCE.png" alt="Harvard University Logo">
                                </div>
                                <h3>Harvard University</h3>
                                <p>Division of Continuing Education</p>
                                <p>DGMD S-14: Wearable Devices & Computer Vision (Grade: A)</p>
                                <p>Jun 2025 – Aug 2025</p>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip for details</span>
                            </div>
                            <div class="card-back">
                                <h4>Harvard Extension Studies</h4>
                                <h5>Advanced Coursework:</h5>
                                <ul>
                                    <li>DGMD S-14: Wearable Devices & Computer Vision (Grade: A)</li>
                                    <li>Focus on AI & Human-Computer Interaction</li>
                                    <li>Research collaboration opportunities</li>
                                </ul>
                                <h5>Academic Details:</h5>
                                <ul>
                                    <li>Graduate-level curriculum</li>
                                    <li>Concurrent with UMD studies</li>
                                    <li>Cross-institutional research</li>
                                    <li>Cambridge, MA</li>
                                </ul>
                                <span class="flip-icon"><i class="fas fa-redo"></i> Flip back</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="experience" class="reveal">
            <div class="container">
                <h2>Professional Experience</h2>
                <div class="timeline">
                    <!-- PRG/UMD Research Assistant -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/umd_logo.png" alt="UMD PRG" class="timeline-logo" />
                                <div>
                                    <h3>Research Assistant <span class="badge">Part-time</span></h3>
                                    <h4>
                                        <a href="https://robotics.umd.edu/facilities/perception-and-robotics-group" target="_blank">
                                            Perception & Robotics Group, University of Maryland
                                        </a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> College Park, MD</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Sept 2025 – Present</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Benchmarked <strong>RoHM, FinePOSE, NLF</strong> on VioDat dataset; produced MPJPE/PCK/AUC baselines with full diagnostics.</li>
                                <li class="visible">Mapped failure modes (bow-hand occlusion, rapid wrist, off-axis drift) and linked to missing temporal/audio cues.</li>
                                <li class="visible">Developing <strong>audio-conditioned temporal refinement</strong> (bi-GRUs + attention) for expressive violin motion.</li>
                                <li class="hidden">Enhancing VioPose (audiovisual 4D pose) for violin performance; synced and preprocessed multimodal recordings for calibrated SMPL-X retargeting.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    <!-- Harvard Experience -->
                    <div class="timeline-item" data-aos="fade-left">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/Harvard%20AI.jpg" alt="Harvard Logo" class="timeline-logo" />
                                <div>
                                    <h3>AI Research Intern <span class="badge">Full Time</span></h3>
                                    <h4>
                                        <a href="https://wang.hms.harvard.edu/team/suryodaya-bikram-shahi/" target="_blank">Harvard Ophthalmology AI & Robotics Lab</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Boston, MA</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> May 2025 – Sept 2025</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Co-built <strong>VISTA</strong>, a multimodal (RGB, spatial audio, IMU, eye-tracking) dataset with precise cross-sensor sync.</li>
                                <li class="visible">Designed <strong>action-grounded annotations</strong> enabling next-best-action assistance; shipped privacy-first pipeline (VRS chunking, IRB workflows, blur filters, Label Studio UI).</li>
                                <li class="visible">Built <strong>on-device captioning/navigation</strong> with sub-second latency on Meta Aria glasses; benchmarked Qwen-VL, LLaVA, SEED-LLaMA; mapped failure modes; tools open-sourced.</li>
                                <li class="hidden">Deployed VLMs (DeepSeek, Qwen-VL, LLaVA) on Meta Aria Gen-1; Qwen-VL best under motion blur (+23% grounding).</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    
                    <!-- DTU Experience -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/dtu_logo.png" alt="DTU Logo" class="timeline-logo" />
                                <div>
                                    <h3>Undergraduate Research <span class="badge">Part-time</span></h3>
                                    <h4>
                                        <a href="https://www.dtu.ac.in/" target="_blank">Delhi Technological University</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> New Delhi, India</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Jan 2022 – Jun 2023</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Conducted a <strong>systematic review</strong> of 43 deep-learning methods for trajectory prediction.</li>
                                <li class="visible">Designed a <strong>three-axis taxonomy</strong> (social-awareness, output type, DL technique).</li>
                                <li class="visible">Contributed <strong>NGSIM-based comparative analysis</strong> and identified gaps in socially aware prediction.</li>
                                <li class="hidden">Co-authored Springer ICICNIS chapter on vehicle trajectory prediction (CNN-LSTM; +14% accuracy).</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    
                    <!-- Macquarie University Experience -->
                    <div class="timeline-item" data-aos="fade-left">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/Macquarie.png" alt="Macquarie Logo" class="timeline-logo" />
                                <div>
                                    <h3>Research Intern <span class="badge">Remote</span></h3>
                                    <h4>
                                        <a href="https://www.mq.edu.au/" target="_blank">Macquarie University</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Sydney, Australia</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> Jul 2023 – Oct 2023</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Co-created <strong>ENeMeme</strong>: one of the first Nepali-English multimodal meme sentiment datasets (5,000+ items).</li>
                                <li class="visible">Built annotation pipeline: text normalization, code-switch handling, visual filtering, sarcasm/toxicity tagging.</li>
                                <li class="visible">Developed multimodal baselines (mBERT + CNN visual features), improving cross-lingual robustness by <strong>+17%</strong>.</li>
                                <li class="hidden">Integrated fairness audits for dialectal bias, slang coverage, and cultural-context handling.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                    <!-- Nepal Electricity Authority Experience -->
                    <div class="timeline-item" data-aos="fade-right">
                        <div class="timeline-dot"></div>
                        <div class="timeline-content">
                            <div class="timeline-header">
                                <img src="images/NEA.png" alt="NEA Logo" class="timeline-logo" />
                                <div>
                                    <h3>IT Intern <span class="badge">On-site</span></h3>
                                    <h4>
                                        <a href="https://nea.org.np/" target="_blank">Nepal Electricity Authority (Government Entity)</a>
                                        <span class="location"><i class="fas fa-map-marker-alt"></i> Kathmandu, Nepal</span>
                                    </h4>
                                    <p class="timeline-date"><i class="fas fa-calendar-alt"></i> May 2022 – July 2022</p>
                                </div>
                            </div>
                            <ul class="timeline-bullets">
                                <li class="visible">Centralized inventory operations with predictive analytics, optimizing accuracy and resource distribution across multiple power stations.</li>
                                <li class="visible">Created predictive model using classification algorithms like Random Forest and resource allocation on a Kaggle dataset for demand forecasting.</li>
                                <li class="visible">Developed data-driven solutions for optimizing electrical grid operations and improving energy distribution efficiency.</li>
                                <li class="hidden">Applied K-Means Clustering techniques to enhance inventory management, improving process efficiency by 50% and reducing operational costs.</li>
                                <li class="hidden">Implemented machine learning models for predictive maintenance of electrical infrastructure, reducing downtime and improving service reliability.</li>
                                <li class="hidden">Collaborated with engineering teams to integrate AI solutions into existing power grid management systems for real-time monitoring and control.</li>
                            </ul>
                            <button class="show-more-btn" onclick="toggleBullets(this)">Show more</button>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="projects" class="reveal">
            <div class="container">
                <h2>Current Projects</h2>
                <p class="projects-intro">Building the future of assistive AI through cutting-edge research in vision-language systems, wearable technology, and edge computing. Each project represents a step toward more accessible and intelligent human-computer interaction.</p>
                <div class="project-grid">
                    <!-- VioPose Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-image-container">
                            <img src="images/VioPose.png" alt="VioPose: Violin Performance 4D Pose Estimation" class="project-main-image">
                        </div>
                        <div class="project-header">
                            <h4 class="project-title">VioPose: Audio-Conditioned Pose Refinement</h4>
                            <p class="project-subtitle">Audio-Conditioned Pose Refinement for Violin Motion</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">University of Maryland</span>
                            <span class="lab">Perception & Robotics Group (Prof. Cornelia Fermüller)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Multimodal Learning</span>
                            <span class="tag">4D Pose Estimation</span>
                            <span class="tag">Audiovisual Fusion</span>
                            <span class="tag">Human Motion</span>
                            <span class="tag">Computer Vision</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>Largest calibrated violin-playing dataset</strong> with video, audio, and 3D motion capture. 
                            Hierarchical multimodal network estimating <strong>fine-grained motions</strong> (vibrato ≈ 10mm) and 
                            <strong>large motions</strong> (bowing) simultaneously, outperforming SoTA visual-only methods.
                        </div>
                        
                        <p class="project-description">
                            Novel multimodal network that hierarchically estimates 4D human pose (3D pose over time) for violin performance 
                            by leveraging the direct causal relationship between music and human motions. Addresses challenges of occlusions, 
                            partial views, and fast subtle movements (e.g., vibrato) that visual-only methods fail to capture. High-level 
                            features cascade to low-level features with Bayesian updates, producing accurate pose sequences for precise 
                            motion analysis. Published at WACV 2025.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>Outperforms state-of-the-art visual-only pose estimation methods</li>
                                    <li>Successfully captures fine-grained vibrato motion (≈10mm perturbation) and large bowing motions</li>
                                    <li>Collected largest and most diverse calibrated violin-playing dataset (12 performers, 4 camera views)</li>
                                    <li>Hierarchical architecture with single-modality encoders, hierarchy module, and mixing module</li>
                                    <li>Code and dataset released for research community benefit</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://sj-yoo.info/viopose/" class="btn-link" target="_blank">
                                <i class="fas fa-globe"></i> Project Page
                            </a>
                            <a href="https://github.com/SirAlchemist1" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> Code
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-file-alt"></i> Paper (WACV 2025)
                            </a>
                        </div>
                    </div>
                    
                    <!-- Aria Glasses Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">Aria Glasses on Qwen</h4>
                            <p class="project-subtitle">Real-Time Vision-Language Pipeline for Meta Aria Gen-1</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">Harvard Medical School</span>
                            <span class="lab">AI & Robotics Lab (Dr. Mengyu Wang)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Wearable AI</span>
                            <span class="tag">Vision-Language Models</span>
                            <span class="tag">On-Device Inference</span>
                            <span class="tag">Assistive Technology</span>
                            <span class="tag">Real-Time Processing</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>Privacy-preserving, real-time scene captioning</strong> on Meta Aria (Gen 1), 
                            <strong>reducing end-to-end latency by 35%</strong> with spoken feedback and fully on-device inference.
                        </div>
                        
                        <p class="project-description">
                            On-device assistive captioning system for Meta Aria Gen-1 smart glasses integrating Qwen-VL for real-time 
                            scene understanding. Delivered privacy-preserving captioning with spoken feedback, enabling fully on-device 
                            inference without cloud dependency. Optimized for low-latency performance, reducing end-to-end latency by 
                            35% compared to baseline implementations.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>35% reduction in end-to-end latency compared to baseline implementations</li>
                                    <li>Privacy-preserving architecture with fully on-device inference</li>
                                    <li>Real-time scene captioning with spoken feedback</li>
                                    <li>Successfully deployed on Meta Aria Gen-1 hardware</li>
                                    <li>Open-source implementation available on GitHub</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/SirAlchemist1/Aria-glasses-on-Qwen" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </div>
                    </div>
                    
                    <!-- Tiny-ACE Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">TINYACE: Bounded Working-Memory Context Engineering</h4>
                            <p class="project-subtitle">Lightweight Adaptation of ACE for Small Language Models with Strategic Forgetting</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">University of Maryland</span>
                            <span class="lab">With Sathwik H. Naik, Archit Harsh</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Small Language Models</span>
                            <span class="tag">Agentic Context Engineering</span>
                            <span class="tag">Working Memory</span>
                            <span class="tag">Edge Computing</span>
                            <span class="tag">Self-Improvement</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>Capacity Sweet Spot identified:</strong> TINYACE improves mid-sized edge-scale models (Phi-3 Mini 3.8B) by 
                            <strong>+4% Accuracy</strong> (37/50 → 39/50) on SciQ benchmark. Introduces bounded working-memory playbook with 
                            strategic forgetting to keep context within fixed token budget (e.g., 512 tokens).
                        </div>
                        
                        <p class="project-description">
                            Lightweight adaptation of Agentic Context Engineering (ACE) for Small Language Models (SLMs) with limited context 
                            windows and strict latency constraints. TINYACE introduces a bounded working-memory playbook that caps context size 
                            and utilizes a feedback loop updating memory primarily on failures to prevent redundancy. Evaluated across three model 
                            scales (1.1B, 3.8B, 7B) on SciQ test split, demonstrating that failure tracking is the most critical component for 
                            accuracy, and simple FIFO eviction often outperforms complex scoring for SLMs.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>Multi-scale evaluation: Tested across 1.1B (TinyLlama), 3.8B (Phi-3 Mini), and 7B (Mistral) models</li>
                                    <li>Identified Capacity Sweet Spot: +4% accuracy improvement for mid-sized models (3.8B)</li>
                                    <li>Failure tracking identified as most critical component (70% vs 78% with failure tracking)</li>
                                    <li>Simple FIFO eviction achieves best performance, matching complex utility scoring</li>
                                    <li>Bounded working-memory prevents context saturation while maintaining reasoning capabilities</li>
                                    <li>Comprehensive ablation study isolating impact of recency bias, failure tracking, and vagueness detection</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/SirAlchemist1/edge-slm-ace" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                            <a href="https://arxiv.org/abs/2510.04618" class="btn-link" target="_blank">
                                <i class="fas fa-file-alt"></i> ArXiv
                            </a>
                        </div>
                    </div>
                    
                    <!-- HC-TAP Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-header">
                            <h4 class="project-title">HC-TAP: Healthcare Text Analytics</h4>
                            <p class="project-subtitle">Serverless NLP Pipeline for Clinical Notes Entity Extraction</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">AWS Serverless Architecture</span>
                            <span class="lab">Healthcare Analytics Platform</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">NLP</span>
                            <span class="tag">AWS Serverless</span>
                            <span class="tag">Healthcare Analytics</span>
                            <span class="tag">Entity Extraction</span>
                            <span class="tag">Streamlit Dashboard</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>HIPAA-aligned, serverless NLP pipeline</strong> using AWS Lambda, Comprehend Medical, Athena, and 
                            Streamlit dashboard; achieved <strong>p95 ≤ 1.5s latency</strong> and high recall on Problems/Medications.
                        </div>
                        
                        <p class="project-description">
                            HIPAA-aligned serverless NLP pipeline extracting medical entities from free-text clinical notes using AWS 
                            Comprehend Medical. Architecture: S3 → Lambda → Comprehend Medical → JSONL storage → Athena DDL for SQL queries. 
                            Built Streamlit analytics dashboard with KPIs (latency p50/p95, error rates), top-10 entity rankings, and CSV 
                            exports. Achieved p95 ≤ 1.5s latency with high recall on Problems/Medications entities.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>HIPAA-aligned architecture ensuring compliance and data security</li>
                                    <li>Achieved p95 ≤ 1.5s latency with high recall on medical entities</li>
                                    <li>Streamlit dashboard with KPIs, top entities, and CSV exports</li>
                                    <li>Serverless architecture: Lambda, S3, Comprehend Medical, Athena</li>
                                    <li>Extracts Problems/Diagnoses, Medications, and Tests entities</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/withsivram/hc-tap" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> GitHub
                            </a>
                        </div>
                    </div>
                    
                    <!-- VISTA Project Card -->
                    <div class="project-card vista-card">
                        <div class="project-image-container">
                            <img src="images/VISTA.png" alt="VISTA: Assistive Vision Dataset" class="project-main-image">
                        </div>
                        <div class="project-header">
                            <h4 class="project-title">VISTA: Assistive Vision Dataset</h4>
                            <p class="project-subtitle">Multimodal Egocentric Dataset for Low-Vision AI</p>
                        </div>
                        
                        <div class="project-affiliation">
                            <span class="institution">Harvard Medical School</span>
                            <span class="lab">AI & Robotics Lab (Dr. Mengyu Wang)</span>
                        </div>
                        
                        <div class="project-tags">
                            <span class="tag">Assistive AI</span>
                            <span class="tag">Dataset Engineering</span>
                            <span class="tag">Wearable Sensing</span>
                            <span class="tag">Privacy-by-Design</span>
                            <span class="tag">Human-Centered AI</span>
                        </div>
                        
                        <div class="project-highlight">
                            <strong>1,000+ egocentric recordings</strong> from Meta Project Aria glasses, 
                            <strong>9 task categories</strong> (navigation, hazard detection, text understanding), 
                            <strong>6 multimodal streams</strong> with &lt;1ms synchronization.
                        </div>
                        
                        <p class="project-description">
                            Led data collection and engineering design of the first multimodal dataset specifically designed 
                            for assistive AI benchmarking with visually impaired users. Coordinated egocentric multimodal 
                            capture (RGB video, IMU, spatial audio, SLAM, eye-tracking), built preprocessing pipeline with 
                            privacy-by-design (automated face/license plate blurring), and developed chunked loading architecture 
                            for efficient handling of 4GB+ .vrs files.
                        </p>
                        
                        <div class="project-details">
                            <div class="detail-item">
                                <span class="detail-label">Key Achievements:</span>
                                <ul class="detail-list">
                                    <li>Sub-second on-device inference latency (NVIDIA Jetson)</li>
                                    <li>Harvard IRB compliance with ethical safeguards throughout workflow</li>
                                    <li>Adaptive preprocessing (gamma correction, exposure balancing) for lighting variability</li>
                                    <li>Custom Label Studio interfaces for action-level ground truth annotation</li>
                                    <li>Planned open-source release on HuggingFace for community benchmarking</li>
                                </ul>
                            </div>
                        </div>
                        
                        <div class="project-links">
                            <a href="https://github.com/Harvard-AI-and-Robotics-Lab" class="btn-link" target="_blank">
                                <i class="fab fa-github"></i> Lab GitHub
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-file"></i> Technical Report
                            </a>
                            <a href="#" class="btn-link" target="_blank">
                                <i class="fas fa-database"></i> Dataset (HuggingFace)
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="skills" class="reveal">
            <div id="skills-particles"></div>
            <div class="container">
                <h2>My Skills</h2>
                <div class="skills-filter-tabs">
                    <button class="filter-tab active" data-category="all">All</button>
                    <button class="filter-tab" data-category="programming">Programming</button>
                    <button class="filter-tab" data-category="advanced-ml">ML/AI</button>
                    <button class="filter-tab" data-category="data-tools">Data Tools</button>
                    <button class="filter-tab" data-category="robotics-vision">Robotics</button>
                    <button class="filter-tab" data-category="research-methods">Research</button>
                    <button class="filter-tab" data-category="academic-writing">Academic</button>
                </div>
                <div class="skills-grid">
                    <div class="skill-hex" data-category="programming">
                        <div class="hexagon">
                            <i class="fas fa-code"></i>
                        </div>
                        <h3>Programming Languages</h3>
                    </div>
                    <div class="skill-hex" data-category="datascience">
                        <div class="hexagon">
                            <i class="fas fa-brain"></i>
                        </div>
                        <h3>Data Science & ML</h3>
                    </div>
                    <div class="skill-hex" data-category="tools">
                        <div class="hexagon">
                            <i class="fas fa-tools"></i>
                        </div>
                        <h3>Tools & Technologies</h3>
                    </div>
                    <div class="skill-hex" data-category="software">
                        <div class="hexagon">
                            <i class="fas fa-laptop-code"></i>
                        </div>
                        <h3>Software Development</h3>
                    </div>
                    <div class="skill-hex" data-category="research">
                        <div class="hexagon">
                            <i class="fas fa-search"></i>
                        </div>
                        <h3>Research & Analysis</h3>
                    </div>
                    <div class="skill-hex" data-category="collaboration">
                        <div class="hexagon">
                            <i class="fas fa-users"></i>
                        </div>
                        <h3>Collaboration & Leadership</h3>
                    </div>
                    <div class="skill-hex" data-category="advanced-ml">
                        <div class="hexagon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <h3>Advanced AI/ML & Robotics</h3>
                    </div>
                    <div class="skill-hex" data-category="data-tools">
                        <div class="hexagon">
                            <i class="fas fa-database"></i>
                        </div>
                        <h3>Data & Databases</h3>
                    </div>
                    <div class="skill-hex" data-category="research-methods">
                        <div class="hexagon">
                            <i class="fas fa-microscope"></i>
                        </div>
                        <h3>Research Methodologies</h3>
                    </div>
                    <div class="skill-hex" data-category="academic-writing">
                        <div class="hexagon">
                            <i class="fas fa-pen-fancy"></i>
                        </div>
                        <h3>Academic Writing</h3>
                    </div>
                </div>
            </div>
            <div id="skill-details" class="skill-details">
                <h3 id="skill-category-title"></h3>
                <ul id="skill-list"></ul>
                <button id="close-skill-details">&times;</button>
            </div>
        </section>

        <section id="awards" class="reveal">
            <div class="container">
                <h2>Awards & Recognition</h2>
                <div class="awards-carousel">
                    <div class="award-badge featured" data-aos="fade-up">
                        <div class="badge-icon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <div class="badge-content">
                            <h3>Perplexity.ai Campus Partner</h3>
                            <p>University of Maryland, College Park</p>
                            <span class="badge-detail">Leading AI Research & Student Engagement</span>
                            <a href="https://www.perplexity.ai/studentonboarding" target="_blank" class="badge-link">View Program</a>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="100">
                        <div class="badge-icon">
                            <i class="fas fa-user-check"></i>
                        </div>
                        <div class="badge-content">
                            <h3>ACM TheWebConf 2025 Reviewer</h3>
                            <p>Workshop MM4SG</p>
                            <span class="badge-detail">2 Papers Reviewed</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="200">
                        <div class="badge-icon">
                            <i class="fas fa-graduation-cap"></i>
                        </div>
                        <div class="badge-content">
                            <h3>ICCR Scholar</h3>
                            <p>2019–2023</p>
                            <span class="badge-detail">Full Scholarship (DTU)</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="300">
                        <div class="badge-icon">
                            <i class="fas fa-rocket"></i>
                        </div>
                        <div class="badge-content">
                            <h3>CTO - Kaushala</h3>
                            <p>EdTech Startup</p>
                            <span class="badge-detail">6 School Partnerships</span>
                        </div>
                    </div>
                    
                    <div class="award-badge" data-aos="fade-up" data-aos-delay="400">
                        <div class="badge-icon">
                            <i class="fas fa-medal"></i>
                        </div>
                        <div class="badge-content">
                            <h3>Gold Medalist</h3>
                            <p>TechFest IIT Bombay</p>
                            <span class="badge-detail">Science Olympiad 2018</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="publications" class="reveal">
            <div class="container">
                <h2>Research Publications & Presentations</h2>
                <p class="publications-intro">Advancing the frontiers of AI through peer-reviewed research in computer vision, multimodal learning, and assistive technologies. My work contributes to both academic knowledge and practical applications for accessibility and human-computer interaction.</p>
                <div class="research-highlights">
                    <div class="highlight-card">
                        <i class="fas fa-chart-line"></i>
                        <span>+4% accuracy improvement (TINYACE)</span>
                    </div>
                    <div class="highlight-card">
                        <i class="fas fa-users"></i>
                        <span>3+ research collaborations</span>
                    </div>
                    <div class="highlight-card">
                        <i class="fas fa-graduation-cap"></i>
                        <span>3 papers in preparation</span>
                    </div>
                </div>
                    
                <div class="publications-grid">
                    <div class="publication-category">
                        <h3><i class="fas fa-file-alt"></i> Published Papers</h3>
                        <div class="publication-list">
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>Deep Learning Methods for Vehicle Trajectory Prediction</h4>
                                    <span class="publication-status">Published</span>
                                </div>
                                <p class="publication-authors">Shiwakoti, S., Shahi, S., & Singh, P.</p>
                                <p class="publication-venue">Springer ICICNIS 2024 · DOI: 10.1007/978-981-99-6586-1_37</p>
                                <div class="publication-links">
                                    <a href="https://link.springer.com/chapter/10.1007/978-981-99-6586-1_37" target="_blank" class="paper-link">Springer Chapter</a>
                                </div>
                                <div class="publication-abstract">
                                    <p>Co-authored paper on deep learning and IoT integration for autonomous vehicles. CNN-LSTM approach achieved +14% accuracy in trajectory prediction.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VTP.png" alt="Deep Learning Methods for Vehicle Trajectory Prediction">
                                    <div class="preview-overlay">
                                        <a href="https://link.springer.com/chapter/10.1007/978-981-99-6586-1_37" target="_blank" class="preview-link">
                                            <i class="fas fa-external-link-alt"></i>
                                            View Paper
                                        </a>
                                    </div>
                                    <p class="preview-caption">Deep Learning Methods for Vehicle Trajectory Prediction - Springer ICICNIS 2023</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="publication-category">
                        <h3><i class="fas fa-microphone"></i> In Preparation</h3>
                        <div class="publication-list">
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>VioPose: Audio-Conditioned Pose Refinement for Violin Motion</h4>
                                    <span class="publication-status">In Prep</span>
                                </div>
                                <p class="publication-authors">Perception and Robotics Group UMD</p>
                                <p class="publication-venue">Target: CVPR 2026</p>
                                <div class="publication-abstract">
                                    <p>Audio-conditioned pose refinement for violin motion; enhancing VioPose (audiovisual 4D pose) for violin performance with <strong>+12–15% pose fidelity</strong> and calibrated SMPL-X retargeting.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VioPose.png" alt="VioPose Research Visualization">
                                    <div class="preview-overlay">
                                        <a href="#" class="preview-link" onclick="showComingSoon()">
                                            <i class="fas fa-file-alt"></i>
                                            Coming Soon
                                        </a>
                                    </div>
                                    <p class="preview-caption">VioPose: Audiovisual 4D Human Pose for Violin Performance</p>
                                </div>
                            </div>
                    
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>VISTA: Action-Grounded Egocentric Dataset for Assistive AI</h4>
                                    <span class="publication-status">In Prep</span>
                                </div>
                                <p class="publication-authors">Yeh, J., Shahi, S., & Wang, M.</p>
                                <p class="publication-venue">Targets: <em>npj Digital Medicine</em> or <em>The Lancet Digital Health</em></p>
                                <div class="publication-abstract">
                                    <p><strong>1000+ real scenarios</strong> + <em>Action-Truth</em> labels for assistive action evaluation using Meta Aria Gen-1 glasses.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/VISTA.png" alt="VISTA Dataset Visualization">
                                    <div class="preview-overlay">
                                        <a href="#" class="preview-link" onclick="showComingSoon()">
                                            <i class="fas fa-file-alt"></i>
                                            Coming Soon
                                        </a>
                                    </div>
                                    <p class="preview-caption">VISTA: Action-Grounded Egocentric Dataset for Assistive AI</p>
                                </div>
                            </div>
                    
                            <div class="publication-item">
                                <div class="publication-header">
                                    <h4>TINYACE: Bounded Working-Memory Context Engineering for SLMs</h4>
                                    <span class="publication-status">In Prep</span>
                                </div>
                                <p class="publication-authors">Shahi, S., Naik, S. H., & Harsh, A.</p>
                                <p class="publication-venue">University of Maryland, College Park</p>
                                <div class="publication-abstract">
                                    <p>Lightweight adaptation of ACE for Small Language Models with strategic forgetting. <strong>+4% accuracy</strong> improvement for mid-sized models (Phi-3 Mini 3.8B) on SciQ benchmark. Identifies "Capacity Sweet Spot" where bounded working-memory playbook effectively bridges reasoning gaps.</p>
                                </div>
                                <div class="paper-preview">
                                    <img src="images/TinyAce.png" alt="TINYACE: Bounded Working-Memory Context Engineering">
                                    <div class="preview-overlay">
                                        <a href="TinyAce%20Paper.pdf" target="_blank" class="preview-link">
                                            <i class="fas fa-file-alt"></i>
                                            View Paper (PDF)
                                        </a>
                                    </div>
                                    <p class="preview-caption">TINYACE: Bounded Working-Memory Context Engineering for SLMs</p>
                                </div>
                                <div class="publication-links">
                                    <a href="TinyAce%20Paper.pdf" target="_blank" class="paper-link">View Paper (PDF)</a>
                                    <a href="https://github.com/SirAlchemist1/edge-slm-ace" target="_blank" class="paper-link">GitHub</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="contact" class="reveal">
            <div class="container">
                <h2 class="section-title">Let's Connect</h2>
                <p class="contact-intro">Interested in collaborating on AI research, discussing assistive technologies, or exploring opportunities in vision-language systems? I'd love to hear from you.</p>
                <div class="contact-wrapper">
                    <div class="contact-info">
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fas fa-envelope"></i>
                            </div>
                            <div class="contact-details">
                                <h3>Email</h3>
                                <p><a href="mailto:sshahi20@umd.edu">sshahi20@umd.edu</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fas fa-phone"></i>
                            </div>
                            <div class="contact-details">
                                <h3>Phone</h3>
                                <p><a href="tel:+16674457800">(667) 445-7800</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fab fa-linkedin"></i>
                            </div>
                            <div class="contact-details">
                                <h3>LinkedIn</h3>
                                <p><a href="https://www.linkedin.com/in/suryodaya-bikram-shahi" target="_blank">Suryodaya Bikram Shahi</a></p>
                            </div>
                        </div>
                        <div class="contact-card">
                            <div class="contact-icon">
                                <i class="fab fa-github"></i>
                            </div>
                            <div class="contact-details">
                                <h3>GitHub</h3>
                                <p><a href="https://github.com/SirAlchemist1" target="_blank">SirAlchemist1</a></p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="footer-content">
            <div class="footer-contact-card">
                <div class="footer-avatar">
                    <img src="images/profilepic-image.jpg" alt="Suryodaya Bikram Shahi">
                </div>
                <div class="footer-info">
                    <h3>Suryodaya Bikram Shahi</h3>
                    <p class="footer-tagline">Let's build something impactful together.</p>
                    <div class="footer-contact-details">
                        <div class="contact-item">
                            <i class="fas fa-envelope"></i>
                            <a href="mailto:sshahi20@umd.edu">sshahi20@umd.edu</a>
                        </div>
                        <div class="contact-item">
                            <i class="fas fa-map-marker-alt"></i>
                            <span>College Park, MD</span>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2024 Suryodaya Bikram Shahi. All rights reserved.</p>
                <div class="social-icons">
                    <a href="https://github.com/SirAlchemist1" target="_blank" class="social-link github-link" aria-label="GitHub">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-github"></i>
                            <span class="social-tooltip">GitHub</span>
                        </div>
                    </a>
                    <a href="https://www.linkedin.com/in/suryodaya-bikram-shahi-051a4b234?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B%2FV1W%2By3DTIC%2Bm8HbWyE%2FCQ%3D%3D" target="_blank" class="social-link linkedin-link" aria-label="LinkedIn">
                        <div class="social-icon-wrapper">
                            <i class="fab fa-linkedin"></i>
                            <span class="social-tooltip">LinkedIn</span>
                        </div>
                    </a>
                    <a href="https://x.com/SuryodayaShahi" target="_blank" class="social-link twitter-link" aria-label="X (Twitter)">
                        <div class="social-icon-wrapper">
                            <svg width="1.1em" height="1.1em" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg" style="vertical-align:middle;"><path d="M299.5 0h220.6l180.6 273.2L885.2 0H1200L753.7 623.6 1200 1227H979.4L779.2 972.2 564.7 1227H250.2l464.2-627.2L0 0h299.5Zm77.2 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6ZM180.6 109.2l423.2 573.2-66.2 89.6-424.2-573.2 67.2-89.6Z" fill="currentColor"/></svg>
                            <span class="social-tooltip">X (Twitter)</span>
                        </div>
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <div id="floating-action-button">
        <i class="fas fa-arrow-up"></i>
    </div>

    <div class="modal" id="project-modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h2 id="modal-title"></h2>
            <p id="modal-description"></p>
            <a id="modal-link" href="#" target="_blank" class="btn primary-btn">View on GitHub</a>
        </div>
    </div>

    <div class="modal" id="resume-modal">
        <div class="modal-content resume-modal-content">
            <span class="close" onclick="closeResumeModal()">&times;</span>
            <h2>My Résumé</h2>
            <div class="resume-selector">
                <div class="resume-tabs">
                    <button class="resume-tab active" onclick="switchResume('academic')" id="academic-tab">
                        <i class="fas fa-graduation-cap"></i> Academic CV
                    </button>
                    <button class="resume-tab" onclick="switchResume('industry')" id="industry-tab">
                        <i class="fas fa-briefcase"></i> Industry Resume
                    </button>
                </div>
            </div>
            <div class="resume-options">
                <div class="resume-preview">
                    <embed id="resume-embed" src="PhD_application_CV%20Suryodaya.pdf" type="application/pdf" width="100%" height="500px">
                </div>
                <div class="resume-actions">
                    <a id="download-link" href="PhD_application_CV%20Suryodaya.pdf" class="btn primary-btn" download>
                        <i class="fas fa-download"></i> Download PDF
                    </a>
                    <a id="open-link" href="PhD_application_CV%20Suryodaya.pdf" class="btn secondary-btn" target="_blank">
                        <i class="fas fa-external-link-alt"></i> Open in New Tab
                    </a>
                </div>
            </div>
        </div>
    </div>

    <script src="script.js"></script>
    
    <script>
    // Dark mode functionality
    function toggleTheme() {
        const body = document.body;
        const themeToggleText = document.querySelector('.theme-toggle-text');
        const themeToggleIcon = document.querySelector('.theme-toggle-icon');
        
        if (body.getAttribute('data-theme') === 'dark') {
            body.removeAttribute('data-theme');
            themeToggleText.textContent = 'Dark';
            themeToggleIcon.innerHTML = '<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>';
            localStorage.setItem('theme', 'light');
        } else {
            body.setAttribute('data-theme', 'dark');
            themeToggleText.textContent = 'Light';
            themeToggleIcon.innerHTML = '<circle cx="12" cy="12" r="5"></circle><path d="m12 1 0 6m0 6 0 6M4.22 4.22l1.42 1.42m11.72 11.72 1.42 1.42M1 12h6m6 0h6M4.22 19.78l1.42-1.42m11.72-11.72 1.42-1.42"></path>';
            localStorage.setItem('theme', 'dark');
        }
    }

    // Initialize theme - default to dark mode
    function initializeTheme() {
        const savedTheme = localStorage.getItem('theme');
        
        // Default to dark mode unless user has explicitly chosen light mode
        if (savedTheme !== 'light') {
            document.body.setAttribute('data-theme', 'dark');
            document.querySelector('.theme-toggle-text').textContent = 'Light';
            document.querySelector('.theme-toggle-icon').innerHTML = '<circle cx="12" cy="12" r="5"></circle><path d="m12 1 0 6m0 6 0 6M4.22 4.22l1.42 1.42m11.72 11.72 1.42 1.42M1 12h6m6 0h6M4.22 19.78l1.42-1.42m11.72-11.72 1.42-1.42"></path>';
            localStorage.setItem('theme', 'dark');
        }
    }

    // Initialize theme on page load
    document.addEventListener('DOMContentLoaded', initializeTheme);

    // Listen for system theme changes
    window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
        if (!localStorage.getItem('theme')) {
            if (e.matches) {
                document.body.setAttribute('data-theme', 'dark');
            } else {
                document.body.removeAttribute('data-theme');
            }
        }
    });

    // Resume switching functionality
    function switchResume(type) {
        const academicTab = document.getElementById('academic-tab');
        const industryTab = document.getElementById('industry-tab');
        const resumeEmbed = document.getElementById('resume-embed');
        const downloadLink = document.getElementById('download-link');
        const openLink = document.getElementById('open-link');
        
        // Update tab states
        if (type === 'academic') {
            academicTab.classList.add('active');
            industryTab.classList.remove('active');
            resumeEmbed.src = 'PhD_application_CV%20Suryodaya.pdf';
            downloadLink.href = 'PhD_application_CV%20Suryodaya.pdf';
            openLink.href = 'PhD_application_CV%20Suryodaya.pdf';
        } else {
            industryTab.classList.add('active');
            academicTab.classList.remove('active');
            resumeEmbed.src = 'Industry%20resume%20.pdf';
            downloadLink.href = 'Industry%20resume%20.pdf';
            openLink.href = 'Industry%20resume%20.pdf';
        }
    }
    </script>
</body>
</html>